{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295f6f93",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py:42\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# from tensorflow.python import keras\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:95\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:387\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 387\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_server_lib\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompress\u001b[39m(element):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwrapt\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:36\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"## Functions for working with arbitrarily nested sequences of elements.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03mNOTE(mrry): This fork of the `tensorflow.python.util.nest` module\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m   arrays.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m _sparse_tensor\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op_callbacks\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:69\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator_utils\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecation\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_utils\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lock_util\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:66\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=unused-import (used in doctests)\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_python_api_dispatcher \u001b[38;5;28;01mas\u001b[39;00m _api_dispatcher\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_decorator\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_dev = x_train[-10000:]\n",
    "y_dev = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# one-hot format\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "y_dev = np_utils.to_categorical(y_dev, 10)\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_dev = x_dev / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb86a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# L2 regularization, Adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time:', time.time() - start)\n",
    "\n",
    "print('train loss:', model_fit.history['loss'][-1])\n",
    "print('train accuracy:', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss:', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy:', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# Dropout regularization(0.2), Adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# Early stopping(patience=5), adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "# early stopping\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "best_test_loss, best_test_acc = best_model.evaluate(x_test, y_test)\n",
    "print('best test model loss: ', best_test_loss)\n",
    "print('best test model accuracy: ', best_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "# L2 regularization, batch norm, adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b54d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "# L2 regularization, Adam optimizer\n",
    "\n",
    "batchSize = 256\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time:', time.time() - start)\n",
    "\n",
    "print('train loss:', model_fit.history['loss'][-1])\n",
    "print('train accuracy:', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss:', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy:', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "# Dropout regularization(0.2), Adam optimizer\n",
    "\n",
    "batchSize = 256\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f6266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "# Early stopping(patience=5), adam optimizer\n",
    "\n",
    "batchSize = 256\n",
    "epoch = 50\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "# early stopping\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "best_test_loss, best_test_acc = best_model.evaluate(x_test, y_test)\n",
    "print('best test model loss: ', best_test_loss)\n",
    "print('best test model accuracy: ', best_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe314002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "# L2 regularization, batch norm, adam optimizer\n",
    "\n",
    "batchSize = 256\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb861020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "# L2 regularization, Adagrad optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71afbd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10\n",
    "# L2 regularization, Stochastic gradient descent optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ecc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11\n",
    "# L2 regularization, RMS prop optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12\n",
    "# Early stopping(patience=10), adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 100\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "# early stopping\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "best_test_loss, best_test_acc = best_model.evaluate(x_test, y_test)\n",
    "print('best test model loss: ', best_test_loss)\n",
    "print('best test model accuracy: ', best_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0835fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13\n",
    "# Early stopping(patience=10), L2 regularization, adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 100\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512,  kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(512,  kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "# early stopping\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "best_test_loss, best_test_acc = best_model.evaluate(x_test, y_test)\n",
    "print('best test model loss: ', best_test_loss)\n",
    "print('best test model accuracy: ', best_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c594552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14\n",
    "# Early stopping(patience=20), L2 regularization, adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 1000\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "# early stopping\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "best_test_loss, best_test_acc = best_model.evaluate(x_test, y_test)\n",
    "print('best test model loss: ', best_test_loss)\n",
    "print('best test model accuracy: ', best_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a78ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# L2 regularization, Adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time:', time.time() - start)\n",
    "\n",
    "print('train loss:', model_fit.history['loss'][-1])\n",
    "print('train accuracy:', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss:', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy:', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d631fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# Dropout regularization(0.2), Adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# Early stopping(patience=5), adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "# early stopping\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "best_test_loss, best_test_acc = best_model.evaluate(x_test, y_test)\n",
    "print('best test model loss: ', best_test_loss)\n",
    "print('best test model accuracy: ', best_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3bf4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "# L2 regularization, batch norm, adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "# L2 regularization, Adam optimizer\n",
    "\n",
    "batchSize = 256\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time:', time.time() - start)\n",
    "\n",
    "print('train loss:', model_fit.history['loss'][-1])\n",
    "print('train accuracy:', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss:', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy:', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "# Dropout regularization(0.2), Adam optimizer\n",
    "\n",
    "batchSize = 256\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "# Early stopping(patience=5), adam optimizer\n",
    "\n",
    "batchSize = 256\n",
    "epoch = 50\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "# early stopping\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "best_test_loss, best_test_acc = best_model.evaluate(x_test, y_test)\n",
    "print('best test model loss: ', best_test_loss)\n",
    "print('best test model accuracy: ', best_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "# L2 regularization, batch norm, adam optimizer\n",
    "\n",
    "batchSize = 256\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b074e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "# L2 regularization, Adagrad optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d348e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10\n",
    "# L2 regularization, Stochastic gradient descent optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea657cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11\n",
    "# L2 regularization, RMS prop optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 50\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev))\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('test loss:', test_loss)\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4ed51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12\n",
    "# Early stopping(patience=10), adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 100\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "# early stopping\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "best_test_loss, best_test_acc = best_model.evaluate(x_test, y_test)\n",
    "print('best test model loss: ', best_test_loss)\n",
    "print('best test model accuracy: ', best_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac6103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13\n",
    "# Early stopping(patience=10), L2 regularization, adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 100\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512,  kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "# early stopping\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "best_test_loss, best_test_acc = best_model.evaluate(x_test, y_test)\n",
    "print('best test model loss: ', best_test_loss)\n",
    "print('best test model accuracy: ', best_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a396965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14\n",
    "# Early stopping(patience=20), L2 regularization, adam optimizer\n",
    "\n",
    "batchSize = 128\n",
    "epoch = 1000\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "# DEFINE MODEL\n",
    "# 512 relu\n",
    "# 10 softmax\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# TRAIN\n",
    "# early stopping\n",
    "model_fit = model.fit(x_train, y_train,\n",
    "                      batch_size=batchSize, epochs=epoch, verbose=1,\n",
    "                      validation_data=(x_dev, y_dev), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print('training time: ', time.time() - start)\n",
    "\n",
    "print('train loss: ', model_fit.history['loss'][-1])\n",
    "print('train accuracy: ', model_fit.history['accuracy'][-1])\n",
    "\n",
    "print('dev loss: ', model_fit.history['val_loss'][-1])\n",
    "print('dev accuracy: ', model_fit.history['val_accuracy'][-1])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_fit.history['val_loss'])\n",
    "plt.title('Val Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_fit.history['val_accuracy'])\n",
    "plt.title('Val Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "best_test_loss, best_test_acc = best_model.evaluate(x_test, y_test)\n",
    "print('best test model loss: ', best_test_loss)\n",
    "print('best test model accuracy: ', best_test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
